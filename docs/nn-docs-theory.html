<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Theory Guide - Neural Networks Suite</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        :root {
            --primary: #9333ea;
            --secondary: #ec4899;
            --accent: #a78bfa;
            --dark: #0a0a0a;
            --light: #f9fafb;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            background: linear-gradient(135deg, #0a0a0a 0%, #1a1a2e 100%);
            color: var(--light);
            min-height: 100vh;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 2rem;
        }

        .header {
            text-align: center;
            margin-bottom: 3rem;
            animation: fadeInDown 0.8s ease;
        }

        .header h1 {
            font-size: 3rem;
            background: linear-gradient(135deg, #9333ea 0%, #ec4899 100%);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
            margin-bottom: 1rem;
        }

        .nav-tabs {
            display: flex;
            gap: 1rem;
            margin-bottom: 2rem;
            flex-wrap: wrap;
            justify-content: center;
        }

        .nav-tab {
            padding: 0.75rem 1.5rem;
            background: rgba(147, 51, 234, 0.1);
            border: 1px solid rgba(147, 51, 234, 0.3);
            border-radius: 0.5rem;
            color: var(--accent);
            cursor: pointer;
            transition: all 0.3s ease;
        }

        .nav-tab:hover, .nav-tab.active {
            background: var(--primary);
            color: white;
            transform: translateY(-2px);
        }

        .content-section {
            background: rgba(147, 51, 234, 0.05);
            border: 1px solid rgba(147, 51, 234, 0.2);
            border-radius: 1rem;
            padding: 2rem;
            margin-bottom: 2rem;
            display: none;
        }

        .content-section.active {
            display: block;
            animation: fadeInUp 0.5s ease;
        }

        h2 {
            color: var(--accent);
            margin-bottom: 1.5rem;
            font-size: 2rem;
        }

        h3 {
            color: #c4b5fd;
            margin: 1.5rem 0 1rem;
            font-size: 1.5rem;
        }

        p {
            line-height: 1.8;
            color: #e9d5ff;
            margin-bottom: 1rem;
        }

        .formula {
            background: rgba(0, 0, 0, 0.3);
            padding: 1rem;
            border-radius: 0.5rem;
            margin: 1rem 0;
            font-family: 'Courier New', monospace;
            color: #fbbf24;
            overflow-x: auto;
        }

        .code-block {
            background: rgba(0, 0, 0, 0.5);
            border: 1px solid rgba(147, 51, 234, 0.3);
            border-radius: 0.5rem;
            padding: 1rem;
            margin: 1rem 0;
            overflow-x: auto;
        }

        .code-block pre {
            color: #e9d5ff;
            font-family: 'Courier New', monospace;
            font-size: 0.9rem;
        }

        .diagram {
            background: rgba(147, 51, 234, 0.05);
            border: 1px solid rgba(147, 51, 234, 0.2);
            border-radius: 0.5rem;
            padding: 2rem;
            margin: 1rem 0;
            text-align: center;
        }

        .feature-list {
            list-style: none;
            margin: 1rem 0;
        }

        .feature-list li {
            padding: 0.5rem 0;
            color: #e9d5ff;
            display: flex;
            align-items: center;
            gap: 0.5rem;
        }

        .feature-list li::before {
            content: '▸';
            color: var(--primary);
            font-weight: bold;
        }

        @keyframes fadeInDown {
            from {
                opacity: 0;
                transform: translateY(-20px);
            }
            to {
                opacity: 1;
                transform: translateY(0);
            }
        }

        @keyframes fadeInUp {
            from {
                opacity: 0;
                transform: translateY(20px);
            }
            to {
                opacity: 1;
                transform: translateY(0);
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1><i class="fas fa-book"></i> Theory Guide</h1>
            <p style="color: #a78bfa;">Deep Learning Theory and Mathematical Foundations</p>
        </div>

        <div class="nav-tabs">
            <div class="nav-tab active" onclick="showSection('fundamentals')">Fundamentals</div>
            <div class="nav-tab" onclick="showSection('architectures')">Architectures</div>
            <div class="nav-tab" onclick="showSection('attention')">Attention Mechanisms</div>
            <div class="nav-tab" onclick="showSection('training')">Training Theory</div>
            <div class="nav-tab" onclick="showSection('optimization')">Optimization</div>
        </div>

        <div class="content-section active" id="fundamentals">
            <h2>Neural Network Fundamentals</h2>
            
            <h3>The Perceptron</h3>
            <p>The foundation of neural networks begins with the perceptron, a simple linear classifier that forms the basis of modern deep learning.</p>
            
            <div class="formula">
                y = σ(Wx + b)
                
                where:
                - W: weight matrix
                - x: input vector
                - b: bias term
                - σ: activation function
            </div>

            <h3>Activation Functions</h3>
            <p>Non-linear activation functions enable neural networks to learn complex patterns:</p>
            
            <ul class="feature-list">
                <li><strong>ReLU:</strong> f(x) = max(0, x)</li>
                <li><strong>Sigmoid:</strong> f(x) = 1 / (1 + e^(-x))</li>
                <li><strong>Tanh:</strong> f(x) = (e^x - e^(-x)) / (e^x + e^(-x))</li>
                <li><strong>GELU:</strong> f(x) = x * Φ(x)</li>
                <li><strong>Swish:</strong> f(x) = x * sigmoid(x)</li>
            </ul>

            <h3>Backpropagation</h3>
            <p>The core algorithm for training neural networks through gradient descent:</p>
            
            <div class="formula">
                ∂L/∂W = ∂L/∂y · ∂y/∂W
                
                Chain Rule Application:
                - Forward pass: compute outputs
                - Backward pass: compute gradients
                - Update weights: W = W - η∇L
            </div>
        </div>

        <div class="content-section" id="architectures">
            <h2>Neural Network Architectures</h2>
            
            <h3>Convolutional Neural Networks (CNNs)</h3>
            <p>Specialized for processing grid-like data such as images:</p>
            
            <div class="code-block">
                <pre>
class ConvBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, 3, padding=1)
        self.bn = nn.BatchNorm2d(out_channels)
        self.relu = nn.ReLU(inplace=True)
        
    def forward(self, x):
        return self.relu(self.bn(self.conv(x)))
                </pre>
            </div>

            <h3>Vision Transformers (ViT)</h3>
            <p>Applying transformer architecture to computer vision:</p>
            
            <ul class="feature-list">
                <li>Patch Embedding: Divide image into patches</li>
                <li>Position Encoding: Add spatial information</li>
                <li>Multi-Head Self-Attention: Global receptive field</li>
                <li>MLP Head: Classification layer</li>
            </ul>

            <div class="formula">
                Attention(Q, K, V) = softmax(QK^T / √d_k)V
            </div>

            <h3>Hybrid Architectures</h3>
            <p>Combining CNNs and Transformers for optimal performance:</p>
            
            <ul class="feature-list">
                <li>CNN backbone for local feature extraction</li>
                <li>Transformer layers for global context</li>
                <li>Best of both worlds: efficiency and accuracy</li>
            </ul>
        </div>

        <div class="content-section" id="attention">
            <h2>Attention Mechanisms</h2>
            
            <h3>Channel Attention (SE Blocks)</h3>
            <p>Squeeze-and-Excitation blocks recalibrate channel-wise feature responses:</p>
            
            <div class="formula">
                1. Squeeze: z_c = F_sq(u_c) = (1/HW) Σ Σ u_c(i,j)
                2. Excitation: s = σ(W_2 · ReLU(W_1 · z))
                3. Scale: x̃_c = s_c · u_c
            </div>

            <h3>Spatial Attention</h3>
            <p>Focus on where to attend in the spatial dimensions:</p>
            
            <div class="code-block">
                <pre>
class SpatialAttention(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv = nn.Conv2d(2, 1, 7, padding=3)
        self.sigmoid = nn.Sigmoid()
        
    def forward(self, x):
        avg_out = torch.mean(x, dim=1, keepdim=True)
        max_out, _ = torch.max(x, dim=1, keepdim=True)
        x = torch.cat([avg_out, max_out], dim=1)
        x = self.conv(x)
        return self.sigmoid(x)
                </pre>
            </div>

            <h3>CBAM (Convolutional Block Attention Module)</h3>
            <p>Combines both channel and spatial attention mechanisms:</p>
            
            <ul class="feature-list">
                <li>Sequential application of channel and spatial attention</li>
                <li>Adaptive feature refinement</li>
                <li>Improved feature representation</li>
                <li>Minimal computational overhead</li>
            </ul>
        </div>

        <div class="content-section" id="training">
            <h2>Training Theory</h2>
            
            <h3>Loss Functions</h3>
            <p>Objective functions for different tasks:</p>
            
            <ul class="feature-list">
                <li><strong>Cross-Entropy:</strong> Classification tasks</li>
                <li><strong>MSE:</strong> Regression tasks</li>
                <li><strong>Focal Loss:</strong> Class imbalance</li>
                <li><strong>Contrastive Loss:</strong> Similarity learning</li>
            </ul>

            <h3>Regularization Techniques</h3>
            <p>Preventing overfitting and improving generalization:</p>
            
            <div class="diagram">
                <h4>Regularization Methods</h4>
                <ul class="feature-list" style="text-align: left; display: inline-block;">
                    <li>Dropout: Randomly zero activations</li>
                    <li>Weight Decay: L2 penalty on weights</li>
                    <li>Data Augmentation: Increase data diversity</li>
                    <li>Early Stopping: Prevent overtraining</li>
                    <li>Batch Normalization: Normalize activations</li>
                </ul>
            </div>

            <h3>Learning Rate Scheduling</h3>
            <p>Adaptive learning rate strategies:</p>
            
            <div class="formula">
                Cosine Annealing: η_t = η_min + 0.5(η_max - η_min)(1 + cos(πt/T))
                
                Step Decay: η_t = η_0 × γ^(floor(t/step_size))
                
                One Cycle: Triangular schedule with momentum inverse
            </div>
        </div>

        <div class="content-section" id="optimization">
            <h2>Optimization Algorithms</h2>
            
            <h3>Gradient Descent Variants</h3>
            
            <div class="code-block">
                <pre>
# SGD with Momentum
v_t = β × v_{t-1} + η × ∇L
W_t = W_{t-1} - v_t

# Adam Optimizer
m_t = β₁ × m_{t-1} + (1 - β₁) × ∇L
v_t = β₂ × v_{t-1} + (1 - β₂) × (∇L)²
m̂_t = m_t / (1 - β₁^t)
v̂_t = v_t / (1 - β₂^t)
W_t = W_{t-1} - η × m̂_t / (√v̂_t + ε)
                </pre>
            </div>

            <h3>Advanced Optimization</h3>
            <ul class="feature-list">
                <li><strong>AdamW:</strong> Decoupled weight decay</li>
                <li><strong>RAdam:</strong> Rectified Adam</li>
                <li><strong>Lookahead:</strong> Stabilized optimization</li>
                <li><strong>LAMB:</strong> Layer-wise adaptive learning</li>
            </ul>

            <h3>Mixed Precision Training</h3>
            <p>Accelerate training with automatic mixed precision:</p>
            
            <div class="code-block">
                <pre>
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

with autocast():
    output = model(input)
    loss = criterion(output, target)

scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
                </pre>
            </div>
        </div>
    </div>

    <script>
        function showSection(section) {
            // Hide all sections
            document.querySelectorAll('.content-section').forEach(s => {
                s.classList.remove('active');
            });
            
            // Remove active from all tabs
            document.querySelectorAll('.nav-tab').forEach(t => {
                t.classList.remove('active');
            });
            
            // Show selected section
            document.getElementById(section).classList.add('active');
            
            // Mark tab as active
            event.target.classList.add('active');
        }
    </script>
</body>
</html>